apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: cascading-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.5, pipelines.kubeflow.org/pipeline_compilation_time: '2021-07-23T15:12:01.186711',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "test named tuple output",
      "name": "cascading_pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.6.5}
spec:
  entrypoint: cascading-pipeline
  templates:
  - name: add
    container:
      args: [--x, '10.0', --y, '5.0', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      artifacts:
      - {name: add-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def add(x, y):\n    return int(x + y)\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Add", "outputs":
          [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "10.0", "y": "5.0"}'}
  - name: add-2
    container:
      args: [--x, '10.0', --y, '5.0', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      artifacts:
      - {name: add-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def add(x, y):\n    return int(x + y)\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Add", "outputs":
          [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "10.0", "y": "5.0"}'}
  - name: caching-check-4-add
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_add(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 add', description='')\n_parser.add_argument(\"--x\", dest=\"x\"\
        , type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_add(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: caching-check-4-add-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-add-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-add-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-add-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_add(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 add'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_add(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 add", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "add_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def add(x, y):\n    return
          int(x + y)\n\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(str(float_value), str(type(float_value))))\n    return str(float_value)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Add'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-check-4-add-2
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_add(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 add', description='')\n_parser.add_argument(\"--x\", dest=\"x\"\
        , type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_add(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: caching-check-4-add-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-add-2-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-add-2-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-add-2-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_add(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 add'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_add(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 add", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "add_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def add(x, y):\n    return
          int(x + y)\n\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(str(float_value), str(type(float_value))))\n    return str(float_value)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Add'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-check-4-divide
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 divide', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: caching-check-4-divide-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-divide-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-divide-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-divide-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_divide(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 divide'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 divide", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "divide_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-check-4-divide-2
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-multiply-Output}}'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 divide', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-Output}
    outputs:
      parameters:
      - name: caching-check-4-divide-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-divide-2-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-divide-2-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-divide-2-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_divide(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 divide'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 divide", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "divide_bundle_2",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-multiply-Output}}",
          "y": "5.0"}'}
  - name: caching-check-4-divide-3
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 divide', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: caching-check-4-divide-3-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-divide-3-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-divide-3-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-divide-3-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_divide(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 divide'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 divide", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "divide_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-check-4-divide-4
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-multiply-2-Output}}'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 divide', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-2-Output}
    outputs:
      parameters:
      - name: caching-check-4-divide-4-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-divide-4-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-divide-4-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-divide-4-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_divide(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 divide'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 divide", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "divide_bundle_2",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-multiply-2-Output}}",
          "y": "5.0"}'}
  - name: caching-check-4-multiply
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-add-Output}}'
      - --y
      - '{{inputs.parameters.gather-data-4-divide-Output}}'
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_multiply(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 multiply', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_multiply(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-Output}
      - {name: gather-data-4-divide-Output}
    outputs:
      parameters:
      - name: caching-check-4-multiply-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-multiply-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-multiply-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-multiply-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_multiply(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 multiply'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_multiply(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 multiply", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "multiply_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def multiply(x,
          y):\n    return x * y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-add-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-Output}}"}'}
  - name: caching-check-4-multiply-2
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-add-2-Output}}'
      - --y
      - '{{inputs.parameters.gather-data-4-divide-3-Output}}'
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - '----output-paths'
      - /tmp/outputs/use_cache/data
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_check_4_multiply(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None):\n    import\
        \ inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n\
        \    # get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n\
        \    dsdt_params = ['dsdt_bundle_name', 'dsdt_context_name', 'dsdt_s3_url',\
        \ 'dsdt_force_rerun', 'dsdt_use_verbose', 'dsdt_container_used', 'dsdt_container_cmd']\n\
        \    user_kwargs, dsdt_kwargs = [], []\n\n    # grab all input parameters\
        \ and pass them in as kwargs\n    for key in args:                       \
        \             \n        temp = key\n        if key in dsdt_params:\n     \
        \       key = key.replace('dsdt_', '')                  # prefix is droped\
        \ as dsdt params are passed in as dict, no need to worry about collision\n\
        \            dsdt_kwargs.append((key, eval(temp)))\n        else:\n      \
        \      user_kwargs.append((key, eval(temp)))\n    user_kwargs = dict(user_kwargs)\n\
        \    dsdt_kwargs = dict(dsdt_kwargs)                     \n    # inject core\
        \ code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n\
        \        \"\"\"\n        :param user_kwargs: user's parameters for the container\
        \ op to be cached. Caching plugin uses this dictionary\n            as signature\
        \ to determine whether re-execution is necessary\n        :param disdat_kwargs:\
        \ parameters reserved for disdat, including bundle name, context name, s3\
        \ path, etc\n        :return: named tuple\n            use_cache: true if\
        \ re-execution is not needed\n            bundle_id: uuid of the bundle with\
        \ identical signature\n        \"\"\"\n        from disdat import api\n  \
        \      import logging\n        from collections import namedtuple\n      \
        \  import os\n\n        Outputs = namedtuple('Outputs', ['use_cache', 'bundle_id'])\n\
        \n        func_name = caching_check.__name__.upper()\n        force_rerun\
        \ = bool(disdat_kwargs.get('force_rerun', False))\n        if force_rerun:\n\
        \            logging.info('{} - {}'.format(func_name, 'forced rerun'))\n \
        \           return Outputs(use_cache=False, bundle_id='')\n\n        bundle_name\
        \ = disdat_kwargs.get('bundle_name')\n        context_name = disdat_kwargs.get('context_name')\n\
        \        s3_url = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n \
        \       logging.info('{} - parameters received: {}, {}'.format(func_name,\
        \ user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt init\")       \
        \                               # disdat is initialized before first execution\n\
        \        api.context(context_name)                                   # set\
        \ disdat context\n        api.remote(context_name, remote_context=context_name,\
        \ remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in\
        \ user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        api.pull(context_name,\
        \ bundle_name)                         # pull all bundles with the same bundle\
        \ name\n        bundle = api.search(context_name, processing_name=proc_name)#\
        \ get the one with the same processing bane\n        use_cache, bid = False,\
        \ ''                                  # by default data should be an empty\
        \ dict\n\n        if len(bundle) > 0:                                    \
        \     # right now just compare parameter signature\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle found'))\n            latest_bundle = bundle[0]\
        \                               # could have multiple because of forced reruns\n\
        \            use_cache = True not in [v != latest_bundle.params.get(k, None)\n\
        \                                     for k, v in component_signature.items()]\n\
        \            bid = latest_bundle.uuid\n        else:\n            logging.info('{}\
        \ - {}'.format(func_name, 'bundle not found'))\n\n        return Outputs(use_cache=use_cache,\
        \ bundle_id=bid)\n\n    # call core code here !\n    result = caching_check(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,\
        \ str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n\
        \        raise TypeError('Value \"{}\" has type \"{}\" instead of bool.'.format(str(bool_value),\
        \ str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:\
        \ str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError('Value\
        \ \"{}\" has type \"{}\" instead of str.'.format(str(str_value), str(type(str_value))))\n\
        \    return str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching\
        \ check 4 multiply', description='')\n_parser.add_argument(\"--x\", dest=\"\
        x\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --y\", dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        ----output-paths\", dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args\
        \ = vars(_parser.parse_args())\n_output_files = _parsed_args.pop(\"_output_paths\"\
        , [])\n\n_outputs = caching_check_4_multiply(**_parsed_args)\n\n_output_serializers\
        \ = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-2-Output}
      - {name: gather-data-4-divide-3-Output}
    outputs:
      parameters:
      - name: caching-check-4-multiply-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      - name: caching-check-4-multiply-2-use_cache
        valueFrom: {path: /tmp/outputs/use_cache/data}
      artifacts:
      - {name: caching-check-4-multiply-2-bundle_id, path: /tmp/outputs/bundle_id/data}
      - {name: caching-check-4-multiply-2-use_cache, path: /tmp/outputs/use_cache/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, "----output-paths", {"outputPath":
          "use_cache"}, {"outputPath": "bundle_id"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def caching_check_4_multiply(x=None,
          y=None, dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url =
          None, dsdt_force_rerun = None, dsdt_use_verbose = None, dsdt_container_used
          = None, dsdt_container_cmd = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_check(user_kwargs,\n                      disdat_kwargs):\n        \"\"\"\n        :param
          user_kwargs: user''s parameters for the container op to be cached. Caching
          plugin uses this dictionary\n            as signature to determine whether
          re-execution is necessary\n        :param disdat_kwargs: parameters reserved
          for disdat, including bundle name, context name, s3 path, etc\n        :return:
          named tuple\n            use_cache: true if re-execution is not needed\n            bundle_id:
          uuid of the bundle with identical signature\n        \"\"\"\n        from
          disdat import api\n        import logging\n        from collections import
          namedtuple\n        import os\n\n        Outputs = namedtuple(''Outputs'',
          [''use_cache'', ''bundle_id''])\n\n        func_name = caching_check.__name__.upper()\n        force_rerun
          = bool(disdat_kwargs.get(''force_rerun'', False))\n        if force_rerun:\n            logging.info(''{}
          - {}''.format(func_name, ''forced rerun''))\n            return Outputs(use_cache=False,
          bundle_id='''')\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n        logging.info(''{}
          - parameters received: {}, {}''.format(func_name, user_kwargs, disdat_kwargs))\n\n        os.system(\"dsdt
          init\")                                      # disdat is initialized before
          first execution\n        api.context(context_name)                                   #
          set disdat context\n        api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n        component_signature = {k: str(v) for k, v in
          user_kwargs.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        api.pull(context_name,
          bundle_name)                         # pull all bundles with the same bundle
          name\n        bundle = api.search(context_name, processing_name=proc_name)#
          get the one with the same processing bane\n        use_cache, bid = False,
          ''''                                  # by default data should be an empty
          dict\n\n        if len(bundle) > 0:                                         #
          right now just compare parameter signature\n            logging.info(''{}
          - {}''.format(func_name, ''bundle found''))\n            latest_bundle =
          bundle[0]                               # could have multiple because of
          forced reruns\n            use_cache = True not in [v != latest_bundle.params.get(k,
          None)\n                                     for k, v in component_signature.items()]\n            bid
          = latest_bundle.uuid\n        else:\n            logging.info(''{} - {}''.format(func_name,
          ''bundle not found''))\n\n        return Outputs(use_cache=use_cache, bundle_id=bid)\n\n    #
          call core code here !\n    result = caching_check(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_bool(bool_value: bool) -> str:\n    if isinstance(bool_value,
          str):\n        return bool_value\n    if not isinstance(bool_value, bool):\n        raise
          TypeError(''Value \"{}\" has type \"{}\" instead of bool.''.format(str(bool_value),
          str(type(bool_value))))\n    return str(bool_value)\n\ndef _serialize_str(str_value:
          str) -> str:\n    if not isinstance(str_value, str):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Caching
          check 4 multiply'', description='''')\n_parser.add_argument(\"--x\", dest=\"x\",
          type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=2)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_check_4_multiply(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_bool,\n    _serialize_str,\n\n]\n\nimport os\nfor idx,
          output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}], "name": "Caching check 4 multiply", "outputs": [{"name": "use_cache",
          "type": "Boolean"}, {"name": "bundle_id", "type": "String"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name": "multiply_bundle",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def multiply(x,
          y):\n    return x * y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-add-2-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-3-Output}}"}'}
  - name: caching-push-4-add
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_add(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ add', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_add(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      artifacts:
      - {name: add-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-add-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-add-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_add(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_add(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 add", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "add_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def add(x, y):\n    return int(x + y)\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-push-4-add-2
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_add(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ add', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_add(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      artifacts:
      - {name: add-2-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-add-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-add-2-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_add(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_add(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 add", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "add_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def add(x, y):\n    return int(x + y)\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Add'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-push-4-divide
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ divide', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      artifacts:
      - {name: divide-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-divide-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-divide-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_divide(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 divide", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-push-4-divide-2
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-multiply-Output}}'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ divide', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-Output}
      artifacts:
      - {name: divide-2-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-divide-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-divide-2-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_divide(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 divide", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle_2", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-multiply-Output}}",
          "y": "5.0"}'}
  - name: caching-push-4-divide-3
    container:
      args:
      - --x
      - '10.0'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ divide', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      artifacts:
      - {name: divide-3-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-divide-3-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-divide-3-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_divide(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 divide", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "10.0", "y": "5.0"}'}
  - name: caching-push-4-divide-4
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-multiply-2-Output}}'
      - --y
      - '5.0'
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_divide(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ divide', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-2-Output}
      artifacts:
      - {name: divide-4-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-divide-4-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-divide-4-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_divide(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_divide(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 divide", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle_2", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-multiply-2-Output}}",
          "y": "5.0"}'}
  - name: caching-push-4-multiply
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-add-Output}}'
      - --y
      - '{{inputs.parameters.gather-data-4-divide-Output}}'
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_multiply(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ multiply', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_multiply(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-Output}
      - {name: gather-data-4-divide-Output}
      artifacts:
      - {name: multiply-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-multiply-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-multiply-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_multiply(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_multiply(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 multiply", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "multiply_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def multiply(x, y):\n    return x * y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-add-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-Output}}"}'}
  - name: caching-push-4-multiply-2
    container:
      args:
      - --x
      - '{{inputs.parameters.gather-data-4-add-2-Output}}'
      - --y
      - '{{inputs.parameters.gather-data-4-divide-3-Output}}'
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --reserve-disdat-Output
      - /tmp/inputs/reserve_disdat_Output/data
      - '----output-paths'
      - /tmp/outputs/bundle_id/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def caching_push_4_multiply(x=None, y=None, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, reserve_disdat_Output\
        \ = None):\n    import inspect, json\n    from typing import NamedTuple\n\
        \    frame = inspect.currentframe()\n    # get the list of input parameters\
        \ \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd']\n    user_kwargs, dsdt_kwargs\
        \ = [], []\n\n    # grab all input parameters and pass them in as kwargs\n\
        \    for key in args:                                    \n        temp =\
        \ key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def caching_push(user_kwargs,\n\
        \                     disdat_kwargs):\n        \"\"\"\n        push output\
        \ from user's container to disdat remote repo\n        parameters suppiled\
        \ to user's container will be used as an unique identifier of current execution\n\
        \n        :param user_kwargs: parameters supplied to user's container\n  \
        \      :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the newly created bundle\n        \"\"\"\n \
        \       import logging, os\n        from disdat import api\n        from collections\
        \ import namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get('bundle_name')\n\
        \        context_name = disdat_kwargs.get('context_name')\n        s3_url\
        \ = disdat_kwargs.get('s3_url')\n        use_verbose = bool(disdat_kwargs.get('use_verbose',\
        \ False))\n        container_used = disdat_kwargs.get('container_used', '')\n\
        \        container_cmd = disdat_kwargs.get('container_cmd', '')\n\n      \
        \  input_src_folder = disdat_kwargs.get('input_src_folder', '/tmp/inputs')\n\
        \        zip_file_name = disdat_kwargs.get('zip_file_name', 'data_cache')\n\
        \        assert '.zip' not in zip_file_name, 'zip file name must not include\
        \ file format extension'\n        user_params, user_artifacts = {}, {}\n \
        \       for key, val in user_kwargs.items():                        # retrieve\
        \ the list of input variables\n            if key.startswith('reserve_disdat'):\n\
        \                user_artifacts[key] = val\n            else:\n          \
        \      user_params[key] = val\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = caching_push.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        logging.info('{} - {}'.format(func_name, user_kwargs))\n\
        \        logging.info('{} - {}'.format(func_name, disdat_kwargs))\n\n    \
        \    os.system(\"dsdt init\")\n        api.context(context_name)\n       \
        \ api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, 'data: ' + str(user_artifacts)))\n\
        \        component_signature = {k: str(v) for k, v in user_params.items()}\n\
        \        proc_name = api.Bundle.calc_default_processing_name(bundle_name,\
        \ component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)\
        \ > 0:\n            for key, path in user_artifacts.items():\n           \
        \     src = path\n                dst = path.replace('reserve_disdat_', '').replace('inputs',\
        \ 'copy')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n\
        \                logging.info('{} - {}'.format(func_name,  src + ' copied\
        \ to ' + dst))\n                shutil.copy(src, dst)                    \
        \           # copy data from /tmp/inputs to /tmp/copy\n            logging.info('{}\
        \ - {}'.format(func_name,  os.listdir(input_src_folder.replace('inputs', 'copy'))))\n\
        \            shutil.make_archive(base_name=zip_file_name, format='zip', root_dir=input_src_folder.replace('inputs',\
        \ 'copy'))\n            logging.info('{} - {}'.format(func_name, 'file zipped\
        \ ' + zip_file_name))\n        with api.Bundle(context_name, name=bundle_name,\
        \ processing_name=proc_name) as b:\n            b.add_params(component_signature)\n\
        \            if len(user_artifacts) > 0:                             # it\
        \ is possible that there's no artifacts to save\n                b.add_data(zip_file_name\
        \ + '.zip')\n            b.add_tags({'container_used': container_used, 'container_cmd':\
        \ container_cmd})\n\n        api.commit(context_name, bundle_name)\n     \
        \   api.push(context_name, bundle_name)                         # save the\
        \ bundle to S3\n        logging.info('{} - {}'.format(func_name, 'data saved\
        \ on s3'))\n        Output = namedtuple('output', ['bundle_id'])\n       \
        \ return Output(b.uuid)                                       # return the\
        \ uuid of the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,\
        \ str):\n        raise TypeError('Value \"{}\" has type \"{}\" instead of\
        \ str.'.format(str(str_value), str(type(str_value))))\n    return str_value\n\
        \nimport argparse\n_parser = argparse.ArgumentParser(prog='Caching push 4\
        \ multiply', description='')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\"\
        , dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-bundle-name\", dest=\"dsdt_bundle_name\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\"\
        , dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\"\
        , dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-use-verbose\", dest=\"dsdt_use_verbose\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\"\
        , dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-cmd\", dest=\"dsdt_container_cmd\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --reserve-disdat-Output\", dest=\"reserve_disdat_Output\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\", dest=\"\
        _output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n\
        _output_files = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_multiply(**_parsed_args)\n\
        \n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx,\
        \ output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n\
        \    except OSError:\n        pass\n    with open(output_file, 'w') as f:\n\
        \        f.write(_output_serializers[idx](_outputs[idx]))\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-2-Output}
      - {name: gather-data-4-divide-3-Output}
      artifacts:
      - {name: multiply-2-Output, path: /tmp/inputs/reserve_disdat_Output/data}
    outputs:
      parameters:
      - name: caching-push-4-multiply-2-bundle_id
        valueFrom: {path: /tmp/outputs/bundle_id/data}
      artifacts:
      - {name: caching-push-4-multiply-2-bundle_id, path: /tmp/outputs/bundle_id/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "x"}, "then": ["--x", {"inputValue":
          "x"}]}}, {"if": {"cond": {"isPresent": "y"}, "then": ["--y", {"inputValue":
          "y"}]}}, {"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "reserve_disdat_Output"},
          "then": ["--reserve-disdat-Output", {"inputPath": "reserve_disdat_Output"}]}},
          "----output-paths", {"outputPath": "bundle_id"}], "command": ["sh", "-c",
          "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def caching_push_4_multiply(x=None, y=None,
          dsdt_bundle_name = None, dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun
          = None, dsdt_use_verbose = None, dsdt_container_used = None, dsdt_container_cmd
          = None, reserve_disdat_Output = None):\n    import inspect, json\n    from
          typing import NamedTuple\n    frame = inspect.currentframe()\n    # get
          the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def caching_push(user_kwargs,\n                     disdat_kwargs):\n        \"\"\"\n        push
          output from user''s container to disdat remote repo\n        parameters
          suppiled to user''s container will be used as an unique identifier of current
          execution\n\n        :param user_kwargs: parameters supplied to user''s
          container\n        :param disdat_kwargs: parameters that disdat use to push
          data to repo\n        :return: the uuid of the newly created bundle\n        \"\"\"\n        import
          logging, os\n        from disdat import api\n        from collections import
          namedtuple\n        import shutil\n\n        bundle_name = disdat_kwargs.get(''bundle_name'')\n        context_name
          = disdat_kwargs.get(''context_name'')\n        s3_url = disdat_kwargs.get(''s3_url'')\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        container_used
          = disdat_kwargs.get(''container_used'', '''')\n        container_cmd = disdat_kwargs.get(''container_cmd'',
          '''')\n\n        input_src_folder = disdat_kwargs.get(''input_src_folder'',
          ''/tmp/inputs'')\n        zip_file_name = disdat_kwargs.get(''zip_file_name'',
          ''data_cache'')\n        assert ''.zip'' not in zip_file_name, ''zip file
          name must not include file format extension''\n        user_params, user_artifacts
          = {}, {}\n        for key, val in user_kwargs.items():                        #
          retrieve the list of input variables\n            if key.startswith(''reserve_disdat''):\n                user_artifacts[key]
          = val\n            else:\n                user_params[key] = val\n\n        if
          use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = caching_push.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        logging.info(''{} - {}''.format(func_name, user_kwargs))\n        logging.info(''{}
          - {}''.format(func_name, disdat_kwargs))\n\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, ''data: '' + str(user_artifacts)))\n        component_signature
          = {k: str(v) for k, v in user_params.items()}\n        proc_name = api.Bundle.calc_default_processing_name(bundle_name,
          component_signature, dep_proc_ids={})\n\n        if len(user_artifacts)
          > 0:\n            for key, path in user_artifacts.items():\n                src
          = path\n                dst = path.replace(''reserve_disdat_'', '''').replace(''inputs'',
          ''copy'')\n                os.makedirs(os.path.dirname(dst), exist_ok=True)\n                logging.info(''{}
          - {}''.format(func_name,  src + '' copied to '' + dst))\n                shutil.copy(src,
          dst)                               # copy data from /tmp/inputs to /tmp/copy\n            logging.info(''{}
          - {}''.format(func_name,  os.listdir(input_src_folder.replace(''inputs'',
          ''copy''))))\n            shutil.make_archive(base_name=zip_file_name, format=''zip'',
          root_dir=input_src_folder.replace(''inputs'', ''copy''))\n            logging.info(''{}
          - {}''.format(func_name, ''file zipped '' + zip_file_name))\n        with
          api.Bundle(context_name, name=bundle_name, processing_name=proc_name) as
          b:\n            b.add_params(component_signature)\n            if len(user_artifacts)
          > 0:                             # it is possible that there''s no artifacts
          to save\n                b.add_data(zip_file_name + ''.zip'')\n            b.add_tags({''container_used'':
          container_used, ''container_cmd'': container_cmd})\n\n        api.commit(context_name,
          bundle_name)\n        api.push(context_name, bundle_name)                         #
          save the bundle to S3\n        logging.info(''{} - {}''.format(func_name,
          ''data saved on s3''))\n        Output = namedtuple(''output'', [''bundle_id''])\n        return
          Output(b.uuid)                                       # return the uuid of
          the bundle\n\n    # call core code here !\n    result = caching_push(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return result\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\ndef _serialize_str(str_value: str) -> str:\n    if not isinstance(str_value,
          str):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          str.''.format(str(str_value), str(type(str_value))))\n    return str_value\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Caching push 4 multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--reserve-disdat-Output\",
          dest=\"reserve_disdat_Output\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = caching_push_4_multiply(**_parsed_args)\n\n_output_serializers
          = [\n    _serialize_str,\n\n]\n\nimport os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "optional": true, "type": "Float"}, {"name": "y", "optional": true,
          "type": "Float"}, {"name": "dsdt_bundle_name", "optional": true, "type":
          "str"}, {"name": "dsdt_context_name", "optional": true, "type": "str"},
          {"name": "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "reserve_disdat_Output", "optional": true, "type": "Float"}],
          "name": "Caching push 4 multiply", "outputs": [{"name": "bundle_id", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "multiply_bundle", "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n
          def multiply(x, y):\n    return x * y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True", "x": "{{inputs.parameters.gather-data-4-add-2-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-3-Output}}"}'}
  - name: cascading-pipeline
    dag:
      tasks:
      - {name: caching-check-4-add, template: caching-check-4-add}
      - name: caching-check-4-add-2
        template: caching-check-4-add-2
        dependencies: [validate-container-execution-4]
      - {name: caching-check-4-divide, template: caching-check-4-divide}
      - name: caching-check-4-divide-2
        template: caching-check-4-divide-2
        dependencies: [gather-data-4-multiply]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-Output, value: '{{tasks.gather-data-4-multiply.outputs.parameters.gather-data-4-multiply-Output}}'}
      - name: caching-check-4-divide-3
        template: caching-check-4-divide-3
        dependencies: [validate-container-execution-4]
      - name: caching-check-4-divide-4
        template: caching-check-4-divide-4
        dependencies: [gather-data-4-multiply-2]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-2-Output, value: '{{tasks.gather-data-4-multiply-2.outputs.parameters.gather-data-4-multiply-2-Output}}'}
      - name: caching-check-4-multiply
        template: caching-check-4-multiply
        dependencies: [gather-data-4-add, gather-data-4-divide]
        arguments:
          parameters:
          - {name: gather-data-4-add-Output, value: '{{tasks.gather-data-4-add.outputs.parameters.gather-data-4-add-Output}}'}
          - {name: gather-data-4-divide-Output, value: '{{tasks.gather-data-4-divide.outputs.parameters.gather-data-4-divide-Output}}'}
      - name: caching-check-4-multiply-2
        template: caching-check-4-multiply-2
        dependencies: [gather-data-4-add-2, gather-data-4-divide-3]
        arguments:
          parameters:
          - {name: gather-data-4-add-2-Output, value: '{{tasks.gather-data-4-add-2.outputs.parameters.gather-data-4-add-2-Output}}'}
          - {name: gather-data-4-divide-3-Output, value: '{{tasks.gather-data-4-divide-3.outputs.parameters.gather-data-4-divide-3-Output}}'}
      - name: condition-1
        template: condition-1
        when: '"{{tasks.caching-check-4-add.outputs.parameters.caching-check-4-add-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-add]
      - name: condition-2
        template: condition-2
        when: '"{{tasks.caching-check-4-divide.outputs.parameters.caching-check-4-divide-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-divide]
      - name: condition-3
        template: condition-3
        when: '"{{tasks.caching-check-4-multiply.outputs.parameters.caching-check-4-multiply-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-multiply, gather-data-4-add, gather-data-4-divide]
        arguments:
          parameters:
          - {name: gather-data-4-add-Output, value: '{{tasks.gather-data-4-add.outputs.parameters.gather-data-4-add-Output}}'}
          - {name: gather-data-4-divide-Output, value: '{{tasks.gather-data-4-divide.outputs.parameters.gather-data-4-divide-Output}}'}
      - name: condition-4
        template: condition-4
        when: '"{{tasks.caching-check-4-divide-2.outputs.parameters.caching-check-4-divide-2-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-divide-2, gather-data-4-multiply]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-Output, value: '{{tasks.gather-data-4-multiply.outputs.parameters.gather-data-4-multiply-Output}}'}
      - name: condition-5
        template: condition-5
        when: '"{{tasks.caching-check-4-add-2.outputs.parameters.caching-check-4-add-2-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-add-2]
      - name: condition-6
        template: condition-6
        when: '"{{tasks.caching-check-4-divide-3.outputs.parameters.caching-check-4-divide-3-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-divide-3]
      - name: condition-7
        template: condition-7
        when: '"{{tasks.caching-check-4-multiply-2.outputs.parameters.caching-check-4-multiply-2-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-multiply-2, gather-data-4-add-2, gather-data-4-divide-3]
        arguments:
          parameters:
          - {name: gather-data-4-add-2-Output, value: '{{tasks.gather-data-4-add-2.outputs.parameters.gather-data-4-add-2-Output}}'}
          - {name: gather-data-4-divide-3-Output, value: '{{tasks.gather-data-4-divide-3.outputs.parameters.gather-data-4-divide-3-Output}}'}
      - name: condition-8
        template: condition-8
        when: '"{{tasks.caching-check-4-divide-4.outputs.parameters.caching-check-4-divide-4-use_cache}}"
          == "False"'
        dependencies: [caching-check-4-divide-4, gather-data-4-multiply-2]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-2-Output, value: '{{tasks.gather-data-4-multiply-2.outputs.parameters.gather-data-4-multiply-2-Output}}'}
      - name: gather-data-4-add
        template: gather-data-4-add
        dependencies: [caching-check-4-add, condition-1]
        arguments:
          parameters:
          - {name: caching-check-4-add-bundle_id, value: '{{tasks.caching-check-4-add.outputs.parameters.caching-check-4-add-bundle_id}}'}
          - {name: caching-push-4-add-bundle_id, value: '{{tasks.condition-1.outputs.parameters.caching-push-4-add-bundle_id}}'}
      - name: gather-data-4-add-2
        template: gather-data-4-add-2
        dependencies: [caching-check-4-add-2, condition-5]
        arguments:
          parameters:
          - {name: caching-check-4-add-2-bundle_id, value: '{{tasks.caching-check-4-add-2.outputs.parameters.caching-check-4-add-2-bundle_id}}'}
          - {name: caching-push-4-add-2-bundle_id, value: '{{tasks.condition-5.outputs.parameters.caching-push-4-add-2-bundle_id}}'}
      - name: gather-data-4-divide
        template: gather-data-4-divide
        dependencies: [caching-check-4-divide, condition-2]
        arguments:
          parameters:
          - {name: caching-check-4-divide-bundle_id, value: '{{tasks.caching-check-4-divide.outputs.parameters.caching-check-4-divide-bundle_id}}'}
          - {name: caching-push-4-divide-bundle_id, value: '{{tasks.condition-2.outputs.parameters.caching-push-4-divide-bundle_id}}'}
      - name: gather-data-4-divide-2
        template: gather-data-4-divide-2
        dependencies: [caching-check-4-divide-2, condition-4]
        arguments:
          parameters:
          - {name: caching-check-4-divide-2-bundle_id, value: '{{tasks.caching-check-4-divide-2.outputs.parameters.caching-check-4-divide-2-bundle_id}}'}
          - {name: caching-push-4-divide-2-bundle_id, value: '{{tasks.condition-4.outputs.parameters.caching-push-4-divide-2-bundle_id}}'}
      - name: gather-data-4-divide-3
        template: gather-data-4-divide-3
        dependencies: [caching-check-4-divide-3, condition-6]
        arguments:
          parameters:
          - {name: caching-check-4-divide-3-bundle_id, value: '{{tasks.caching-check-4-divide-3.outputs.parameters.caching-check-4-divide-3-bundle_id}}'}
          - {name: caching-push-4-divide-3-bundle_id, value: '{{tasks.condition-6.outputs.parameters.caching-push-4-divide-3-bundle_id}}'}
      - name: gather-data-4-divide-4
        template: gather-data-4-divide-4
        dependencies: [caching-check-4-divide-4, condition-8]
        arguments:
          parameters:
          - {name: caching-check-4-divide-4-bundle_id, value: '{{tasks.caching-check-4-divide-4.outputs.parameters.caching-check-4-divide-4-bundle_id}}'}
          - {name: caching-push-4-divide-4-bundle_id, value: '{{tasks.condition-8.outputs.parameters.caching-push-4-divide-4-bundle_id}}'}
      - name: gather-data-4-multiply
        template: gather-data-4-multiply
        dependencies: [caching-check-4-multiply, condition-3]
        arguments:
          parameters:
          - {name: caching-check-4-multiply-bundle_id, value: '{{tasks.caching-check-4-multiply.outputs.parameters.caching-check-4-multiply-bundle_id}}'}
          - {name: caching-push-4-multiply-bundle_id, value: '{{tasks.condition-3.outputs.parameters.caching-push-4-multiply-bundle_id}}'}
      - name: gather-data-4-multiply-2
        template: gather-data-4-multiply-2
        dependencies: [caching-check-4-multiply-2, condition-7]
        arguments:
          parameters:
          - {name: caching-check-4-multiply-2-bundle_id, value: '{{tasks.caching-check-4-multiply-2.outputs.parameters.caching-check-4-multiply-2-bundle_id}}'}
          - {name: caching-push-4-multiply-2-bundle_id, value: '{{tasks.condition-7.outputs.parameters.caching-push-4-multiply-2-bundle_id}}'}
      - name: validate-container-execution
        template: validate-container-execution
        dependencies: [gather-data-4-add]
      - name: validate-container-execution-2
        template: validate-container-execution-2
        dependencies: [gather-data-4-divide]
      - name: validate-container-execution-3
        template: validate-container-execution-3
        dependencies: [gather-data-4-multiply]
      - name: validate-container-execution-4
        template: validate-container-execution-4
        dependencies: [gather-data-4-divide-2]
      - name: validate-container-no-execution
        template: validate-container-no-execution
        dependencies: [gather-data-4-add-2, validate-container-execution]
        arguments:
          parameters:
          - {name: validate-container-execution-Output, value: '{{tasks.validate-container-execution.outputs.parameters.validate-container-execution-Output}}'}
      - name: validate-container-no-execution-2
        template: validate-container-no-execution-2
        dependencies: [gather-data-4-divide-3, validate-container-execution-2]
        arguments:
          parameters:
          - {name: validate-container-execution-2-Output, value: '{{tasks.validate-container-execution-2.outputs.parameters.validate-container-execution-2-Output}}'}
      - name: validate-container-no-execution-3
        template: validate-container-no-execution-3
        dependencies: [gather-data-4-multiply-2, validate-container-execution-3]
        arguments:
          parameters:
          - {name: validate-container-execution-3-Output, value: '{{tasks.validate-container-execution-3.outputs.parameters.validate-container-execution-3-Output}}'}
      - name: validate-container-no-execution-4
        template: validate-container-no-execution-4
        dependencies: [gather-data-4-divide-4, validate-container-execution-4]
        arguments:
          parameters:
          - {name: validate-container-execution-4-Output, value: '{{tasks.validate-container-execution-4.outputs.parameters.validate-container-execution-4-Output}}'}
  - name: condition-1
    outputs:
      parameters:
      - name: caching-push-4-add-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-add.outputs.parameters.caching-push-4-add-bundle_id}}'}
    dag:
      tasks:
      - {name: add, template: add}
      - name: caching-push-4-add
        template: caching-push-4-add
        dependencies: [add]
        arguments:
          artifacts:
          - {name: add-Output, from: '{{tasks.add.outputs.artifacts.add-Output}}'}
      - name: noop
        template: noop
        dependencies: [caching-push-4-add]
        arguments:
          parameters:
          - {name: caching-push-4-add-bundle_id, value: '{{tasks.caching-push-4-add.outputs.parameters.caching-push-4-add-bundle_id}}'}
  - name: condition-2
    outputs:
      parameters:
      - name: caching-push-4-divide-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-divide.outputs.parameters.caching-push-4-divide-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-divide
        template: caching-push-4-divide
        dependencies: [divide]
        arguments:
          artifacts:
          - {name: divide-Output, from: '{{tasks.divide.outputs.artifacts.divide-Output}}'}
      - {name: divide, template: divide}
      - name: noop-2
        template: noop-2
        dependencies: [caching-push-4-divide]
        arguments:
          parameters:
          - {name: caching-push-4-divide-bundle_id, value: '{{tasks.caching-push-4-divide.outputs.parameters.caching-push-4-divide-bundle_id}}'}
  - name: condition-3
    inputs:
      parameters:
      - {name: gather-data-4-add-Output}
      - {name: gather-data-4-divide-Output}
    outputs:
      parameters:
      - name: caching-push-4-multiply-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-multiply.outputs.parameters.caching-push-4-multiply-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-multiply
        template: caching-push-4-multiply
        dependencies: [multiply]
        arguments:
          parameters:
          - {name: gather-data-4-add-Output, value: '{{inputs.parameters.gather-data-4-add-Output}}'}
          - {name: gather-data-4-divide-Output, value: '{{inputs.parameters.gather-data-4-divide-Output}}'}
          artifacts:
          - {name: multiply-Output, from: '{{tasks.multiply.outputs.artifacts.multiply-Output}}'}
      - name: multiply
        template: multiply
        arguments:
          parameters:
          - {name: gather-data-4-add-Output, value: '{{inputs.parameters.gather-data-4-add-Output}}'}
          - {name: gather-data-4-divide-Output, value: '{{inputs.parameters.gather-data-4-divide-Output}}'}
      - name: noop-3
        template: noop-3
        dependencies: [caching-push-4-multiply]
        arguments:
          parameters:
          - {name: caching-push-4-multiply-bundle_id, value: '{{tasks.caching-push-4-multiply.outputs.parameters.caching-push-4-multiply-bundle_id}}'}
  - name: condition-4
    inputs:
      parameters:
      - {name: gather-data-4-multiply-Output}
    outputs:
      parameters:
      - name: caching-push-4-divide-2-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-divide-2.outputs.parameters.caching-push-4-divide-2-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-divide-2
        template: caching-push-4-divide-2
        dependencies: [divide-2]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-Output, value: '{{inputs.parameters.gather-data-4-multiply-Output}}'}
          artifacts:
          - {name: divide-2-Output, from: '{{tasks.divide-2.outputs.artifacts.divide-2-Output}}'}
      - name: divide-2
        template: divide-2
        arguments:
          parameters:
          - {name: gather-data-4-multiply-Output, value: '{{inputs.parameters.gather-data-4-multiply-Output}}'}
      - name: noop-4
        template: noop-4
        dependencies: [caching-push-4-divide-2]
        arguments:
          parameters:
          - {name: caching-push-4-divide-2-bundle_id, value: '{{tasks.caching-push-4-divide-2.outputs.parameters.caching-push-4-divide-2-bundle_id}}'}
  - name: condition-5
    outputs:
      parameters:
      - name: caching-push-4-add-2-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-add-2.outputs.parameters.caching-push-4-add-2-bundle_id}}'}
    dag:
      tasks:
      - {name: add-2, template: add-2}
      - name: caching-push-4-add-2
        template: caching-push-4-add-2
        dependencies: [add-2]
        arguments:
          artifacts:
          - {name: add-2-Output, from: '{{tasks.add-2.outputs.artifacts.add-2-Output}}'}
      - name: noop-5
        template: noop-5
        dependencies: [caching-push-4-add-2]
        arguments:
          parameters:
          - {name: caching-push-4-add-2-bundle_id, value: '{{tasks.caching-push-4-add-2.outputs.parameters.caching-push-4-add-2-bundle_id}}'}
  - name: condition-6
    outputs:
      parameters:
      - name: caching-push-4-divide-3-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-divide-3.outputs.parameters.caching-push-4-divide-3-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-divide-3
        template: caching-push-4-divide-3
        dependencies: [divide-3]
        arguments:
          artifacts:
          - {name: divide-3-Output, from: '{{tasks.divide-3.outputs.artifacts.divide-3-Output}}'}
      - {name: divide-3, template: divide-3}
      - name: noop-6
        template: noop-6
        dependencies: [caching-push-4-divide-3]
        arguments:
          parameters:
          - {name: caching-push-4-divide-3-bundle_id, value: '{{tasks.caching-push-4-divide-3.outputs.parameters.caching-push-4-divide-3-bundle_id}}'}
  - name: condition-7
    inputs:
      parameters:
      - {name: gather-data-4-add-2-Output}
      - {name: gather-data-4-divide-3-Output}
    outputs:
      parameters:
      - name: caching-push-4-multiply-2-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-multiply-2.outputs.parameters.caching-push-4-multiply-2-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-multiply-2
        template: caching-push-4-multiply-2
        dependencies: [multiply-2]
        arguments:
          parameters:
          - {name: gather-data-4-add-2-Output, value: '{{inputs.parameters.gather-data-4-add-2-Output}}'}
          - {name: gather-data-4-divide-3-Output, value: '{{inputs.parameters.gather-data-4-divide-3-Output}}'}
          artifacts:
          - {name: multiply-2-Output, from: '{{tasks.multiply-2.outputs.artifacts.multiply-2-Output}}'}
      - name: multiply-2
        template: multiply-2
        arguments:
          parameters:
          - {name: gather-data-4-add-2-Output, value: '{{inputs.parameters.gather-data-4-add-2-Output}}'}
          - {name: gather-data-4-divide-3-Output, value: '{{inputs.parameters.gather-data-4-divide-3-Output}}'}
      - name: noop-7
        template: noop-7
        dependencies: [caching-push-4-multiply-2]
        arguments:
          parameters:
          - {name: caching-push-4-multiply-2-bundle_id, value: '{{tasks.caching-push-4-multiply-2.outputs.parameters.caching-push-4-multiply-2-bundle_id}}'}
  - name: condition-8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-2-Output}
    outputs:
      parameters:
      - name: caching-push-4-divide-4-bundle_id
        valueFrom: {parameter: '{{tasks.caching-push-4-divide-4.outputs.parameters.caching-push-4-divide-4-bundle_id}}'}
    dag:
      tasks:
      - name: caching-push-4-divide-4
        template: caching-push-4-divide-4
        dependencies: [divide-4]
        arguments:
          parameters:
          - {name: gather-data-4-multiply-2-Output, value: '{{inputs.parameters.gather-data-4-multiply-2-Output}}'}
          artifacts:
          - {name: divide-4-Output, from: '{{tasks.divide-4.outputs.artifacts.divide-4-Output}}'}
      - name: divide-4
        template: divide-4
        arguments:
          parameters:
          - {name: gather-data-4-multiply-2-Output, value: '{{inputs.parameters.gather-data-4-multiply-2-Output}}'}
      - name: noop-8
        template: noop-8
        dependencies: [caching-push-4-divide-4]
        arguments:
          parameters:
          - {name: caching-push-4-divide-4-bundle_id, value: '{{tasks.caching-push-4-divide-4.outputs.parameters.caching-push-4-divide-4-bundle_id}}'}
  - name: divide
    container:
      args: [--x, '10.0', --y, '5.0', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      artifacts:
      - {name: divide-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Divide",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "10.0", "y": "5.0"}'}
  - name: divide-2
    container:
      args: [--x, '{{inputs.parameters.gather-data-4-multiply-Output}}', --y, '5.0',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-Output}
    outputs:
      artifacts:
      - {name: divide-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Divide",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "{{inputs.parameters.gather-data-4-multiply-Output}}",
          "y": "5.0"}'}
  - name: divide-3
    container:
      args: [--x, '10.0', --y, '5.0', '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      artifacts:
      - {name: divide-3-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Divide",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "10.0", "y": "5.0"}'}
  - name: divide-4
    container:
      args: [--x, '{{inputs.parameters.gather-data-4-multiply-2-Output}}', --y, '5.0',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-multiply-2-Output}
    outputs:
      artifacts:
      - {name: divide-4-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def divide(x, y):\n    return x / y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Divide",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "{{inputs.parameters.gather-data-4-multiply-2-Output}}",
          "y": "5.0"}'}
  - name: gather-data-4-add
    container:
      args:
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-add-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-add-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_add(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 add', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_add(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-add-bundle_id}
      - {name: caching-push-4-add-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-add-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-add-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_add(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 add'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_add(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 add", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "add_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-add-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-add-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def add(x, y):\n    return
          int(x + y)\n\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(str(float_value), str(type(float_value))))\n    return str(float_value)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Add'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-add-2
    container:
      args:
      - --dsdt-bundle-name
      - add_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def add(x, y):
            return int(x + y)

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Add', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = add(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-add-2-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-add-2-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_add(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 add', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_add(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-add-2-bundle_id}
      - {name: caching-push-4-add-2-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-add-2-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-add-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_add(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 add'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_add(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 add", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "add_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-add-2-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-add-2-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def add(x, y):\n    return
          int(x + y)\n\ndef _serialize_float(float_value: float) -> str:\n    if isinstance(float_value,
          str):\n        return float_value\n    if not isinstance(float_value, (float,
          int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead of
          float.''.format(str(float_value), str(type(float_value))))\n    return str(float_value)\n\nimport
          argparse\n_parser = argparse.ArgumentParser(prog=''Add'', description='''')\n_parser.add_argument(\"--x\",
          dest=\"x\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = add(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-divide
    container:
      args:
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-divide-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-divide-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 divide', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_divide(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-divide-bundle_id}
      - {name: caching-push-4-divide-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-divide-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-divide-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 divide'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_divide(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 divide", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-divide-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-divide-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-divide-2
    container:
      args:
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-divide-2-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-divide-2-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 divide', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_divide(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-divide-2-bundle_id}
      - {name: caching-push-4-divide-2-bundle_id}
    outputs:
      artifacts:
      - {name: gather-data-4-divide-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 divide'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_divide(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 divide", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle_2", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-divide-2-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-divide-2-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-divide-3
    container:
      args:
      - --dsdt-bundle-name
      - divide_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-divide-3-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-divide-3-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 divide', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_divide(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-divide-3-bundle_id}
      - {name: caching-push-4-divide-3-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-divide-3-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-divide-3-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 divide'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_divide(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 divide", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-divide-3-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-divide-3-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-divide-4
    container:
      args:
      - --dsdt-bundle-name
      - divide_bundle_2
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def divide(x, y):
            return x / y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Divide', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = divide(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-divide-4-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-divide-4-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 divide', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_divide(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-divide-4-bundle_id}
      - {name: caching-push-4-divide-4-bundle_id}
    outputs:
      artifacts:
      - {name: gather-data-4-divide-4-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_divide(Output, dsdt_bundle_name = None, dsdt_context_name
          = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =
          None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 divide'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_divide(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 divide", "outputs": [{"name": "Output", "type": "Float"}]}',
        pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "divide_bundle_2", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-divide-4-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-divide-4-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def divide(x,
          y):\n    return x / y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Divide'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = divide(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-multiply
    container:
      args:
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "True"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-multiply-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-multiply-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_multiply(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 multiply', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_multiply(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-multiply-bundle_id}
      - {name: caching-push-4-multiply-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-multiply-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-multiply-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_multiply(Output, dsdt_bundle_name = None,
          dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose
          = None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 multiply'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_multiply(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 multiply", "outputs": [{"name": "Output", "type":
          "Float"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "multiply_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-multiply-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-multiply-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def multiply(x,
          y):\n    return x * y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "True", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: gather-data-4-multiply-2
    container:
      args:
      - --dsdt-bundle-name
      - multiply_bundle
      - --dsdt-context-name
      - cascading_pipeline
      - --dsdt-s3-url
      - s3://mint-ai-disdatnoop-e2e-435945521637
      - --dsdt-force-rerun
      - "False"
      - --dsdt-use-verbose
      - "True"
      - --dsdt-container-used
      - docker.intuit.com/docker-rmt/python:3.8
      - --dsdt-container-cmd
      - |
        sh -ec program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
         def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      - --dsdt-caching-check-uuid
      - '{{inputs.parameters.caching-check-4-multiply-2-bundle_id}}'
      - --dsdt-caching-push-uuid
      - '{{inputs.parameters.caching-push-4-multiply-2-bundle_id}}'
      - --dsdt-output-var-name-list
      - '["Output"]'
      - --Output
      - /tmp/outputs/Output/data
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def _make_parent_dirs_and_return_path(file_path: str):\n    import os\n \
        \   os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return file_path\n\
        \ndef gather_data_4_multiply(Output, dsdt_bundle_name = None, dsdt_context_name\
        \ = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose =\
        \ None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid\
        \ = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n\
        \    import inspect, json\n    from typing import NamedTuple\n    frame =\
        \ inspect.currentframe()\n    # get the list of input parameters \n    args,\
        \ _, _, _ = inspect.getargvalues(frame)\n    dsdt_params = ['dsdt_bundle_name',\
        \ 'dsdt_context_name', 'dsdt_s3_url', 'dsdt_force_rerun', 'dsdt_use_verbose',\
        \ 'dsdt_container_used', 'dsdt_container_cmd', 'dsdt_caching_check_uuid',\
        \ 'dsdt_caching_push_uuid', 'dsdt_output_var_name_list']\n    user_kwargs,\
        \ dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in\
        \ as kwargs\n    for key in args:                                    \n  \
        \      temp = key\n        if key in dsdt_params:\n            key = key.replace('dsdt_',\
        \ '')                  # prefix is droped as dsdt params are passed in as\
        \ dict, no need to worry about collision\n            dsdt_kwargs.append((key,\
        \ eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n\
        \    user_kwargs = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)\
        \                     \n    # inject core code here !\n    def gather_data(user_kwargs,\n\
        \                    disdat_kwargs):\n        \"\"\"\n        determine whether\
        \ the caching wrapper should return the result from user's container (cache\
        \ not used)\n        or re-use a previously executed result identified by\
        \ uuid returned fom cahcing_check()\n\n        :param user_kwargs: should\
        \ be an empty dictionary, used to simplify code generation logic\n       \
        \ :param disdat_kwargs: parameters that disdat use to push data to repo\n\
        \        :return: the uuid of the chosen bundle\n        \"\"\"\n\n      \
        \  from disdat import api\n        import os\n        import shutil\n    \
        \    import logging\n        import re\n\n        bundle_name = disdat_kwargs['bundle_name']\n\
        \        context_name = disdat_kwargs['context_name']\n        s3_url = disdat_kwargs['s3_url']\n\
        \        use_verbose = bool(disdat_kwargs.get('use_verbose', False))\n   \
        \     caching_check_uuid = disdat_kwargs['caching_check_uuid']\n        caching_push_uuid\
        \ = disdat_kwargs['caching_push_uuid']\n        # output_var_names = disdat_kwargs['output_var_name_list']\n\
        \        unzip_path = disdat_kwargs.get('unzip_path', 'unzipped_folder')\n\
        \        output_path = disdat_kwargs.get('output_path', '/tmp/outputs')\n\n\
        \        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n\
        \        else:\n            logging.basicConfig(level=logging.WARNING)\n\n\
        \        func_name = gather_data.__name__\n        logging.info('{} - {}'.format(func_name,\
        \ 'initialized'))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n\
        \        api.remote(context_name, remote_context=context_name, remote_url=s3_url)\n\
        \        logging.info('{} - {}'.format(func_name, user_kwargs))\n        logging.info('{}\
        \ - {}'.format(func_name, disdat_kwargs))\n\n        # aws_access_key_id =\
        \ disdat_kwargs.get('s3_access_key_id', None)\n        # aws_secret_access_key\
        \ = disdat_kwargs.get('s3_secret_access_key_id', None)\n        # aws_session_token\
        \ = disdat_kwargs.get('s3_session_token', None)\n        #\n        # write_to_file\
        \ = (aws_access_key_id is not None) and (aws_secret_access_key is not None)\n\
        \        # if write_to_file:\n        #     logging.info('{} - {}'.format(func_name,\
        \ 'write aws credentials to file'))\n        #     credential_file = os.path.expanduser('~/.aws')\n\
        \        #     os.makedirs(credential_file, exist_ok=True)\n        #    \
        \ with open(credential_file + '/credentials', 'w') as fp:\n        #     \
        \    fp.write('[default]\\n')\n        #         fp.write('aws_access_key_id={}\\\
        n'.format(aws_access_key_id))\n        #         fp.write('aws_secret_access_key={}\\\
        n'.format(aws_secret_access_key))\n        #         if aws_session_token:\n\
        \        #             fp.write('aws_session_token={}'.format(aws_session_token))\n\
        \n        if not re.match(pattern=r'\\{\\{.+\\}\\}', string=caching_push_uuid):\
        \           # use cached bundle if condition is met\n            uuid = caching_push_uuid\n\
        \        else:\n            uuid = caching_check_uuid\n\n        api.pull(context_name,\
        \ uuid=uuid, localize=True)                            # bundle the bundle\
        \ with all its files\n        bundle = api.get(context_name, bundle_name=bundle_name,\
        \ uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file = os.path.join(disdat_dir,\
        \ 'data_cache.zip')\n        if os.path.isdir(unzip_path):               \
        \                                # check if zip file is present\n        \
        \    os.system('rm -r {}'.format(unzip_path))\n        os.makedirs(unzip_path,\
        \ exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,\
        \ extract_dir=unzip_path, format='zip')\n            logging.info('{} - {}'.format(func_name,\
        \ 'unzipped files -' + ','.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,\
        \ dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\
        \n        return uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,\
        \ disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)\
        \ -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)\
        \ == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog='Gather\
        \ data 4 multiply', description='')\n_parser.add_argument(\"--dsdt-bundle-name\"\
        , dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-context-name\", dest=\"dsdt_context_name\",\
        \ type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-s3-url\", dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-force-rerun\", dest=\"dsdt_force_rerun\", type=_deserialize_bool,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\"\
        , dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-container-used\", dest=\"dsdt_container_used\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-container-cmd\", dest=\"dsdt_container_cmd\", type=str, required=False,\
        \ default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\"\
        , dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n\
        _parser.add_argument(\"--dsdt-caching-push-uuid\", dest=\"dsdt_caching_push_uuid\"\
        , type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"\
        --dsdt-output-var-name-list\", dest=\"dsdt_output_var_name_list\", type=json.loads,\
        \ required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\"\
        , dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,\
        \ default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n\
        _outputs = gather_data_4_multiply(**_parsed_args)\n"
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-check-4-multiply-2-bundle_id}
      - {name: caching-push-4-multiply-2-bundle_id}
    outputs:
      parameters:
      - name: gather-data-4-multiply-2-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: gather-data-4-multiply-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [{"if": {"cond": {"isPresent": "dsdt_bundle_name"}, "then": ["--dsdt-bundle-name",
          {"inputValue": "dsdt_bundle_name"}]}}, {"if": {"cond": {"isPresent": "dsdt_context_name"},
          "then": ["--dsdt-context-name", {"inputValue": "dsdt_context_name"}]}},
          {"if": {"cond": {"isPresent": "dsdt_s3_url"}, "then": ["--dsdt-s3-url",
          {"inputValue": "dsdt_s3_url"}]}}, {"if": {"cond": {"isPresent": "dsdt_force_rerun"},
          "then": ["--dsdt-force-rerun", {"inputValue": "dsdt_force_rerun"}]}}, {"if":
          {"cond": {"isPresent": "dsdt_use_verbose"}, "then": ["--dsdt-use-verbose",
          {"inputValue": "dsdt_use_verbose"}]}}, {"if": {"cond": {"isPresent": "dsdt_container_used"},
          "then": ["--dsdt-container-used", {"inputValue": "dsdt_container_used"}]}},
          {"if": {"cond": {"isPresent": "dsdt_container_cmd"}, "then": ["--dsdt-container-cmd",
          {"inputValue": "dsdt_container_cmd"}]}}, {"if": {"cond": {"isPresent": "dsdt_caching_check_uuid"},
          "then": ["--dsdt-caching-check-uuid", {"inputValue": "dsdt_caching_check_uuid"}]}},
          {"if": {"cond": {"isPresent": "dsdt_caching_push_uuid"}, "then": ["--dsdt-caching-push-uuid",
          {"inputValue": "dsdt_caching_push_uuid"}]}}, {"if": {"cond": {"isPresent":
          "dsdt_output_var_name_list"}, "then": ["--dsdt-output-var-name-list", {"inputValue":
          "dsdt_output_var_name_list"}]}}, "--Output", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef gather_data_4_multiply(Output, dsdt_bundle_name = None,
          dsdt_context_name = None, dsdt_s3_url = None, dsdt_force_rerun = None, dsdt_use_verbose
          = None, dsdt_container_used = None, dsdt_container_cmd = None, dsdt_caching_check_uuid
          = None, dsdt_caching_push_uuid = None, dsdt_output_var_name_list = None):\n    import
          inspect, json\n    from typing import NamedTuple\n    frame = inspect.currentframe()\n    #
          get the list of input parameters \n    args, _, _, _ = inspect.getargvalues(frame)\n    dsdt_params
          = [''dsdt_bundle_name'', ''dsdt_context_name'', ''dsdt_s3_url'', ''dsdt_force_rerun'',
          ''dsdt_use_verbose'', ''dsdt_container_used'', ''dsdt_container_cmd'', ''dsdt_caching_check_uuid'',
          ''dsdt_caching_push_uuid'', ''dsdt_output_var_name_list'']\n    user_kwargs,
          dsdt_kwargs = [], []\n\n    # grab all input parameters and pass them in
          as kwargs\n    for key in args:                                    \n        temp
          = key\n        if key in dsdt_params:\n            key = key.replace(''dsdt_'',
          '''')                  # prefix is droped as dsdt params are passed in as
          dict, no need to worry about collision\n            dsdt_kwargs.append((key,
          eval(temp)))\n        else:\n            user_kwargs.append((key, eval(temp)))\n    user_kwargs
          = dict(user_kwargs)\n    dsdt_kwargs = dict(dsdt_kwargs)                     \n    #
          inject core code here !\n    def gather_data(user_kwargs,\n                    disdat_kwargs):\n        \"\"\"\n        determine
          whether the caching wrapper should return the result from user''s container
          (cache not used)\n        or re-use a previously executed result identified
          by uuid returned fom cahcing_check()\n\n        :param user_kwargs: should
          be an empty dictionary, used to simplify code generation logic\n        :param
          disdat_kwargs: parameters that disdat use to push data to repo\n        :return:
          the uuid of the chosen bundle\n        \"\"\"\n\n        from disdat import
          api\n        import os\n        import shutil\n        import logging\n        import
          re\n\n        bundle_name = disdat_kwargs[''bundle_name'']\n        context_name
          = disdat_kwargs[''context_name'']\n        s3_url = disdat_kwargs[''s3_url'']\n        use_verbose
          = bool(disdat_kwargs.get(''use_verbose'', False))\n        caching_check_uuid
          = disdat_kwargs[''caching_check_uuid'']\n        caching_push_uuid = disdat_kwargs[''caching_push_uuid'']\n        #
          output_var_names = disdat_kwargs[''output_var_name_list'']\n        unzip_path
          = disdat_kwargs.get(''unzip_path'', ''unzipped_folder'')\n        output_path
          = disdat_kwargs.get(''output_path'', ''/tmp/outputs'')\n\n        if use_verbose:\n            logging.basicConfig(level=logging.INFO)\n        else:\n            logging.basicConfig(level=logging.WARNING)\n\n        func_name
          = gather_data.__name__\n        logging.info(''{} - {}''.format(func_name,
          ''initialized''))\n        os.system(\"dsdt init\")\n        api.context(context_name)\n        api.remote(context_name,
          remote_context=context_name, remote_url=s3_url)\n        logging.info(''{}
          - {}''.format(func_name, user_kwargs))\n        logging.info(''{} - {}''.format(func_name,
          disdat_kwargs))\n\n        # aws_access_key_id = disdat_kwargs.get(''s3_access_key_id'',
          None)\n        # aws_secret_access_key = disdat_kwargs.get(''s3_secret_access_key_id'',
          None)\n        # aws_session_token = disdat_kwargs.get(''s3_session_token'',
          None)\n        #\n        # write_to_file = (aws_access_key_id is not None)
          and (aws_secret_access_key is not None)\n        # if write_to_file:\n        #     logging.info(''{}
          - {}''.format(func_name, ''write aws credentials to file''))\n        #     credential_file
          = os.path.expanduser(''~/.aws'')\n        #     os.makedirs(credential_file,
          exist_ok=True)\n        #     with open(credential_file + ''/credentials'',
          ''w'') as fp:\n        #         fp.write(''[default]\\n'')\n        #         fp.write(''aws_access_key_id={}\\n''.format(aws_access_key_id))\n        #         fp.write(''aws_secret_access_key={}\\n''.format(aws_secret_access_key))\n        #         if
          aws_session_token:\n        #             fp.write(''aws_session_token={}''.format(aws_session_token))\n\n        if
          not re.match(pattern=r''\\{\\{.+\\}\\}'', string=caching_push_uuid):           #
          use cached bundle if condition is met\n            uuid = caching_push_uuid\n        else:\n            uuid
          = caching_check_uuid\n\n        api.pull(context_name, uuid=uuid, localize=True)                            #
          bundle the bundle with all its files\n        bundle = api.get(context_name,
          bundle_name=bundle_name, uuid=uuid)\n        disdat_dir = bundle.local_dir\n        zip_file
          = os.path.join(disdat_dir, ''data_cache.zip'')\n        if os.path.isdir(unzip_path):                                               #
          check if zip file is present\n            os.system(''rm -r {}''.format(unzip_path))\n        os.makedirs(unzip_path,
          exist_ok=True)\n        if os.path.isfile(zip_file):\n            shutil.unpack_archive(zip_file,
          extract_dir=unzip_path, format=''zip'')\n            logging.info(''{} -
          {}''.format(func_name, ''unzipped files -'' + '',''.join(os.listdir(unzip_path))))\n            shutil.copytree(src=unzip_path,
          dst=output_path, dirs_exist_ok=True)    # copy unzipped folder to /tmp/outputs/\n\n        return
          uuid\n\n    # call core code here !\n    result = gather_data(user_kwargs=user_kwargs,
          disdat_kwargs=dsdt_kwargs)\n    return None\n\ndef _deserialize_bool(s)
          -> bool:\n    from distutils.util import strtobool\n    return strtobool(s)
          == 1\n\nimport json\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Gather
          data 4 multiply'', description='''')\n_parser.add_argument(\"--dsdt-bundle-name\",
          dest=\"dsdt_bundle_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-context-name\",
          dest=\"dsdt_context_name\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-s3-url\",
          dest=\"dsdt_s3_url\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-force-rerun\",
          dest=\"dsdt_force_rerun\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-use-verbose\",
          dest=\"dsdt_use_verbose\", type=_deserialize_bool, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-used\",
          dest=\"dsdt_container_used\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-container-cmd\",
          dest=\"dsdt_container_cmd\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-check-uuid\",
          dest=\"dsdt_caching_check_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-caching-push-uuid\",
          dest=\"dsdt_caching_push_uuid\", type=str, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--dsdt-output-var-name-list\",
          dest=\"dsdt_output_var_name_list\", type=json.loads, required=False, default=argparse.SUPPRESS)\n_parser.add_argument(\"--Output\",
          dest=\"Output\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = gather_data_4_multiply(**_parsed_args)\n"], "image": "docker.intuit.com/docker-rmt/python:3.8"}},
          "inputs": [{"name": "dsdt_bundle_name", "optional": true, "type": "str"},
          {"name": "dsdt_context_name", "optional": true, "type": "str"}, {"name":
          "dsdt_s3_url", "optional": true, "type": "str"}, {"name": "dsdt_force_rerun",
          "optional": true, "type": "bool"}, {"name": "dsdt_use_verbose", "optional":
          true, "type": "bool"}, {"name": "dsdt_container_used", "optional": true,
          "type": "str"}, {"name": "dsdt_container_cmd", "optional": true, "type":
          "str"}, {"name": "dsdt_caching_check_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_caching_push_uuid", "optional": true, "type": "String"},
          {"name": "dsdt_output_var_name_list", "optional": true, "type": "list"}],
          "name": "Gather data 4 multiply", "outputs": [{"name": "Output", "type":
          "Float"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"dsdt_bundle_name":
          "multiply_bundle", "dsdt_caching_check_uuid": "{{inputs.parameters.caching-check-4-multiply-2-bundle_id}}",
          "dsdt_caching_push_uuid": "{{inputs.parameters.caching-push-4-multiply-2-bundle_id}}",
          "dsdt_container_cmd": "sh -ec program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n def multiply(x,
          y):\n    return x * y\n\ndef _serialize_float(float_value: float) -> str:\n    if
          isinstance(float_value, str):\n        return float_value\n    if not isinstance(float_value,
          (float, int)):\n        raise TypeError(''Value \"{}\" has type \"{}\" instead
          of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n",
          "dsdt_container_used": "docker.intuit.com/docker-rmt/python:3.8", "dsdt_context_name":
          "cascading_pipeline", "dsdt_force_rerun": "False", "dsdt_output_var_name_list":
          "[\"Output\"]", "dsdt_s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "dsdt_use_verbose": "True"}'}
  - name: multiply
    container:
      args: [--x, '{{inputs.parameters.gather-data-4-add-Output}}', --y, '{{inputs.parameters.gather-data-4-divide-Output}}',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-Output}
      - {name: gather-data-4-divide-Output}
    outputs:
      artifacts:
      - {name: multiply-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def multiply(x, y):\n    return x * y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Multiply",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "{{inputs.parameters.gather-data-4-add-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-Output}}"}'}
  - name: multiply-2
    container:
      args: [--x, '{{inputs.parameters.gather-data-4-add-2-Output}}', --y, '{{inputs.parameters.gather-data-4-divide-3-Output}}',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def multiply(x, y):
            return x * y

        def _serialize_float(float_value: float) -> str:
            if isinstance(float_value, str):
                return float_value
            if not isinstance(float_value, (float, int)):
                raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
            return str(float_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Multiply', description='')
        _parser.add_argument("--x", dest="x", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--y", dest="y", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = multiply(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_float,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: gather-data-4-add-2-Output}
      - {name: gather-data-4-divide-3-Output}
    outputs:
      artifacts:
      - {name: multiply-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--x", {"inputValue": "x"}, "--y", {"inputValue": "y"}, "----output-paths",
          {"outputPath": "Output"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def multiply(x, y):\n    return x * y\n\ndef _serialize_float(float_value:
          float) -> str:\n    if isinstance(float_value, str):\n        return float_value\n    if
          not isinstance(float_value, (float, int)):\n        raise TypeError(''Value
          \"{}\" has type \"{}\" instead of float.''.format(str(float_value), str(type(float_value))))\n    return
          str(float_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Multiply'',
          description='''')\n_parser.add_argument(\"--x\", dest=\"x\", type=float,
          required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--y\",
          dest=\"y\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = multiply(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_float,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "x", "type": "Float"}, {"name": "y", "type": "Float"}], "name": "Multiply",
          "outputs": [{"name": "Output", "type": "Float"}]}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"x": "{{inputs.parameters.gather-data-4-add-2-Output}}",
          "y": "{{inputs.parameters.gather-data-4-divide-3-Output}}"}'}
  - name: noop
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-add-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-add-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-add-bundle_id}}"}'}
  - name: noop-2
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-divide-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-divide-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-divide-bundle_id}}"}'}
  - name: noop-3
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-multiply-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-multiply-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-multiply-bundle_id}}"}'}
  - name: noop-4
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-divide-2-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-divide-2-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-divide-2-bundle_id}}"}'}
  - name: noop-5
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-add-2-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-add-2-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-add-2-bundle_id}}"}'}
  - name: noop-6
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-divide-3-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-divide-3-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-divide-3-bundle_id}}"}'}
  - name: noop-7
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-multiply-2-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-multiply-2-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-multiply-2-bundle_id}}"}'}
  - name: noop-8
    container:
      args: [--var, '{{inputs.parameters.caching-push-4-divide-4-bundle_id}}']
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def noop(var):
            """
            This no op container does nothing. However, it cannot be removed for the following reason:
                In order to use the output of caching_push outside the condition (which is represented as a group op in kfp),
                the parameter is referenced as task.condition.outputs.parameters.xxx, which again refers to the output of
                caching_push as task.caching_push.outputs.parameters.xxx. However, since caching_push is not used by another
                container, it does not have outputs.parameter but outputs.artifacts.
                Hence, a noop container is used to force the compiler to generate an outputs.parameter section for caching_push
            :param var: pipeline parameter passed in from another container
            :return: nothing
            """
            import logging
            logging.warning('{} - {}'.format('NO_OP',
                                             'this no op container is used to bypass a kfp compiler bug'))

        import argparse
        _parser = argparse.ArgumentParser(prog='Noop', description='This no op container does nothing. However, it cannot be removed for the following reason:')
        _parser.add_argument("--var", dest="var", type=str, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = noop(**_parsed_args)
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: caching-push-4-divide-4-bundle_id}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "This
          no op container does nothing. However, it cannot be removed for the following
          reason:", "implementation": {"container": {"args": ["--var", {"inputValue":
          "var"}], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          noop(var):\n    \"\"\"\n    This no op container does nothing. However,
          it cannot be removed for the following reason:\n        In order to use
          the output of caching_push outside the condition (which is represented as
          a group op in kfp),\n        the parameter is referenced as task.condition.outputs.parameters.xxx,
          which again refers to the output of\n        caching_push as task.caching_push.outputs.parameters.xxx.
          However, since caching_push is not used by another\n        container, it
          does not have outputs.parameter but outputs.artifacts.\n        Hence, a
          noop container is used to force the compiler to generate an outputs.parameter
          section for caching_push\n    :param var: pipeline parameter passed in from
          another container\n    :return: nothing\n    \"\"\"\n    import logging\n    logging.warning(''{}
          - {}''.format(''NO_OP'',\n                                     ''this no
          op container is used to bypass a kfp compiler bug''))\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Noop'', description=''This no op container
          does nothing. However, it cannot be removed for the following reason:'')\n_parser.add_argument(\"--var\",
          dest=\"var\", type=str, required=True, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = noop(**_parsed_args)\n"], "image":
          "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"description":
          "pipeline parameter passed in from another container", "name": "var", "type":
          "String"}], "name": "Noop"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"var": "{{inputs.parameters.caching-push-4-divide-4-bundle_id}}"}'}
  - name: validate-container-execution
    container:
      args: [--bundle-name, add_bundle, --context-name, cascading_pipeline, --s3-url,
        's3://mint-ai-disdatnoop-e2e-435945521637', --younger-than, '1627067520.183891',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_execution(bundle_name,
                                         context_name,
                                         s3_url,
                                         younger_than):
            from disdat import api
            import os
            import logging
            func_name = validate_container_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle= api.get(context_name, bundle_name)
            creation_time = latest_bundle.creation_date

            assert creation_time > younger_than, "task has been skipped unexpectedly!"
            return latest_bundle.uuid

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--younger-than", dest="younger_than", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: validate-container-execution-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: validate-container-execution-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--context-name",
          {"inputValue": "context_name"}, "--s3-url", {"inputValue": "s3_url"}, "--younger-than",
          {"inputValue": "younger_than"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def validate_container_execution(bundle_name,\n                                 context_name,\n                                 s3_url,\n                                 younger_than):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle= api.get(context_name,
          bundle_name)\n    creation_time = latest_bundle.creation_date\n\n    assert
          creation_time > younger_than, \"task has been skipped unexpectedly!\"\n    return
          latest_bundle.uuid\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--younger-than\",
          dest=\"younger_than\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "context_name", "type": "String"},
          {"name": "s3_url", "type": "String"}, {"name": "younger_than", "type": "Float"}],
          "name": "Validate container execution", "outputs": [{"name": "Output", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "add_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "younger_than": "1627067520.183891"}'}
  - name: validate-container-execution-2
    container:
      args: [--bundle-name, divide_bundle, --context-name, cascading_pipeline, --s3-url,
        's3://mint-ai-disdatnoop-e2e-435945521637', --younger-than, '1627067520.310577',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_execution(bundle_name,
                                         context_name,
                                         s3_url,
                                         younger_than):
            from disdat import api
            import os
            import logging
            func_name = validate_container_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle= api.get(context_name, bundle_name)
            creation_time = latest_bundle.creation_date

            assert creation_time > younger_than, "task has been skipped unexpectedly!"
            return latest_bundle.uuid

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--younger-than", dest="younger_than", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: validate-container-execution-2-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: validate-container-execution-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--context-name",
          {"inputValue": "context_name"}, "--s3-url", {"inputValue": "s3_url"}, "--younger-than",
          {"inputValue": "younger_than"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def validate_container_execution(bundle_name,\n                                 context_name,\n                                 s3_url,\n                                 younger_than):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle= api.get(context_name,
          bundle_name)\n    creation_time = latest_bundle.creation_date\n\n    assert
          creation_time > younger_than, \"task has been skipped unexpectedly!\"\n    return
          latest_bundle.uuid\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--younger-than\",
          dest=\"younger_than\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "context_name", "type": "String"},
          {"name": "s3_url", "type": "String"}, {"name": "younger_than", "type": "Float"}],
          "name": "Validate container execution", "outputs": [{"name": "Output", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "divide_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "younger_than": "1627067520.310577"}'}
  - name: validate-container-execution-3
    container:
      args: [--bundle-name, multiply_bundle, --context-name, cascading_pipeline, --s3-url,
        's3://mint-ai-disdatnoop-e2e-435945521637', --younger-than, '1627067520.4470909',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_execution(bundle_name,
                                         context_name,
                                         s3_url,
                                         younger_than):
            from disdat import api
            import os
            import logging
            func_name = validate_container_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle= api.get(context_name, bundle_name)
            creation_time = latest_bundle.creation_date

            assert creation_time > younger_than, "task has been skipped unexpectedly!"
            return latest_bundle.uuid

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--younger-than", dest="younger_than", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: validate-container-execution-3-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: validate-container-execution-3-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--context-name",
          {"inputValue": "context_name"}, "--s3-url", {"inputValue": "s3_url"}, "--younger-than",
          {"inputValue": "younger_than"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def validate_container_execution(bundle_name,\n                                 context_name,\n                                 s3_url,\n                                 younger_than):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle= api.get(context_name,
          bundle_name)\n    creation_time = latest_bundle.creation_date\n\n    assert
          creation_time > younger_than, \"task has been skipped unexpectedly!\"\n    return
          latest_bundle.uuid\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--younger-than\",
          dest=\"younger_than\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "context_name", "type": "String"},
          {"name": "s3_url", "type": "String"}, {"name": "younger_than", "type": "Float"}],
          "name": "Validate container execution", "outputs": [{"name": "Output", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "multiply_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "younger_than": "1627067520.4470909"}'}
  - name: validate-container-execution-4
    container:
      args: [--bundle-name, divide_bundle_2, --context-name, cascading_pipeline, --s3-url,
        's3://mint-ai-disdatnoop-e2e-435945521637', --younger-than, '1627067520.5733728',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_execution(bundle_name,
                                         context_name,
                                         s3_url,
                                         younger_than):
            from disdat import api
            import os
            import logging
            func_name = validate_container_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle= api.get(context_name, bundle_name)
            creation_time = latest_bundle.creation_date

            assert creation_time > younger_than, "task has been skipped unexpectedly!"
            return latest_bundle.uuid

        def _serialize_str(str_value: str) -> str:
            if not isinstance(str_value, str):
                raise TypeError('Value "{}" has type "{}" instead of str.'.format(str(str_value), str(type(str_value))))
            return str_value

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--younger-than", dest="younger_than", type=float, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_str,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    outputs:
      parameters:
      - name: validate-container-execution-4-Output
        valueFrom: {path: /tmp/outputs/Output/data}
      artifacts:
      - {name: validate-container-execution-4-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--context-name",
          {"inputValue": "context_name"}, "--s3-url", {"inputValue": "s3_url"}, "--younger-than",
          {"inputValue": "younger_than"}, "----output-paths", {"outputPath": "Output"}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''disdat'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def validate_container_execution(bundle_name,\n                                 context_name,\n                                 s3_url,\n                                 younger_than):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle= api.get(context_name,
          bundle_name)\n    creation_time = latest_bundle.creation_date\n\n    assert
          creation_time > younger_than, \"task has been skipped unexpectedly!\"\n    return
          latest_bundle.uuid\n\ndef _serialize_str(str_value: str) -> str:\n    if
          not isinstance(str_value, str):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of str.''.format(str(str_value), str(type(str_value))))\n    return
          str_value\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--younger-than\",
          dest=\"younger_than\", type=float, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_str,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "context_name", "type": "String"},
          {"name": "s3_url", "type": "String"}, {"name": "younger_than", "type": "Float"}],
          "name": "Validate container execution", "outputs": [{"name": "Output", "type":
          "String"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "divide_bundle_2", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "younger_than": "1627067520.5733728"}'}
  - name: validate-container-no-execution
    container:
      args: [--bundle-name, add_bundle, --uuid, '{{inputs.parameters.validate-container-execution-Output}}',
        --context-name, cascading_pipeline, --s3-url, 's3://mint-ai-disdatnoop-e2e-435945521637',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_no_execution(bundle_name,
                                            uuid,
                                            context_name,
                                            s3_url):
            from disdat import api
            import os
            import logging
            func_name = validate_container_no_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle = api.get(context_name, bundle_name)

            assert uuid == latest_bundle.uuid, "task has been re-executed unexpectedly!"
            return True

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container no execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--uuid", dest="uuid", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_no_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: validate-container-execution-Output}
    outputs:
      artifacts:
      - {name: validate-container-no-execution-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--uuid", {"inputValue":
          "uuid"}, "--context-name", {"inputValue": "context_name"}, "--s3-url", {"inputValue":
          "s3_url"}, "----output-paths", {"outputPath": "Output"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def validate_container_no_execution(bundle_name,\n                                    uuid,\n                                    context_name,\n                                    s3_url):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_no_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle = api.get(context_name,
          bundle_name)\n\n    assert uuid == latest_bundle.uuid, \"task has been re-executed
          unexpectedly!\"\n    return True\n\ndef _serialize_bool(bool_value: bool)
          -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(str(bool_value), str(type(bool_value))))\n    return
          str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container no execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--uuid\",
          dest=\"uuid\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_no_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "uuid", "type": "String"}, {"name":
          "context_name", "type": "String"}, {"name": "s3_url", "type": "String"}],
          "name": "Validate container no execution", "outputs": [{"name": "Output",
          "type": "Boolean"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "add_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "uuid": "{{inputs.parameters.validate-container-execution-Output}}"}'}
  - name: validate-container-no-execution-2
    container:
      args: [--bundle-name, divide_bundle, --uuid, '{{inputs.parameters.validate-container-execution-2-Output}}',
        --context-name, cascading_pipeline, --s3-url, 's3://mint-ai-disdatnoop-e2e-435945521637',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_no_execution(bundle_name,
                                            uuid,
                                            context_name,
                                            s3_url):
            from disdat import api
            import os
            import logging
            func_name = validate_container_no_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle = api.get(context_name, bundle_name)

            assert uuid == latest_bundle.uuid, "task has been re-executed unexpectedly!"
            return True

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container no execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--uuid", dest="uuid", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_no_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: validate-container-execution-2-Output}
    outputs:
      artifacts:
      - {name: validate-container-no-execution-2-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--uuid", {"inputValue":
          "uuid"}, "--context-name", {"inputValue": "context_name"}, "--s3-url", {"inputValue":
          "s3_url"}, "----output-paths", {"outputPath": "Output"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def validate_container_no_execution(bundle_name,\n                                    uuid,\n                                    context_name,\n                                    s3_url):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_no_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle = api.get(context_name,
          bundle_name)\n\n    assert uuid == latest_bundle.uuid, \"task has been re-executed
          unexpectedly!\"\n    return True\n\ndef _serialize_bool(bool_value: bool)
          -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(str(bool_value), str(type(bool_value))))\n    return
          str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container no execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--uuid\",
          dest=\"uuid\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_no_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "uuid", "type": "String"}, {"name":
          "context_name", "type": "String"}, {"name": "s3_url", "type": "String"}],
          "name": "Validate container no execution", "outputs": [{"name": "Output",
          "type": "Boolean"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "divide_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "uuid": "{{inputs.parameters.validate-container-execution-2-Output}}"}'}
  - name: validate-container-no-execution-3
    container:
      args: [--bundle-name, multiply_bundle, --uuid, '{{inputs.parameters.validate-container-execution-3-Output}}',
        --context-name, cascading_pipeline, --s3-url, 's3://mint-ai-disdatnoop-e2e-435945521637',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_no_execution(bundle_name,
                                            uuid,
                                            context_name,
                                            s3_url):
            from disdat import api
            import os
            import logging
            func_name = validate_container_no_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle = api.get(context_name, bundle_name)

            assert uuid == latest_bundle.uuid, "task has been re-executed unexpectedly!"
            return True

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container no execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--uuid", dest="uuid", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_no_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: validate-container-execution-3-Output}
    outputs:
      artifacts:
      - {name: validate-container-no-execution-3-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--uuid", {"inputValue":
          "uuid"}, "--context-name", {"inputValue": "context_name"}, "--s3-url", {"inputValue":
          "s3_url"}, "----output-paths", {"outputPath": "Output"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def validate_container_no_execution(bundle_name,\n                                    uuid,\n                                    context_name,\n                                    s3_url):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_no_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle = api.get(context_name,
          bundle_name)\n\n    assert uuid == latest_bundle.uuid, \"task has been re-executed
          unexpectedly!\"\n    return True\n\ndef _serialize_bool(bool_value: bool)
          -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(str(bool_value), str(type(bool_value))))\n    return
          str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container no execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--uuid\",
          dest=\"uuid\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_no_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "uuid", "type": "String"}, {"name":
          "context_name", "type": "String"}, {"name": "s3_url", "type": "String"}],
          "name": "Validate container no execution", "outputs": [{"name": "Output",
          "type": "Boolean"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "multiply_bundle", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "uuid": "{{inputs.parameters.validate-container-execution-3-Output}}"}'}
  - name: validate-container-no-execution-4
    container:
      args: [--bundle-name, divide_bundle_2, --uuid, '{{inputs.parameters.validate-container-execution-4-Output}}',
        --context-name, cascading_pipeline, --s3-url, 's3://mint-ai-disdatnoop-e2e-435945521637',
        '----output-paths', /tmp/outputs/Output/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'disdat' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'disdat' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def validate_container_no_execution(bundle_name,
                                            uuid,
                                            context_name,
                                            s3_url):
            from disdat import api
            import os
            import logging
            func_name = validate_container_no_execution.__name__.upper()
            os.system('dsdt init')
            api.context(context_name)
            api.remote(context_name, remote_context=context_name, remote_url=s3_url)
            logging.basicConfig(level=logging.INFO)
            logging.info('{} - {}'.format(func_name, 'disdat initialized'))

            api.pull(context_name, bundle_name=bundle_name, localize=False)
            latest_bundle = api.get(context_name, bundle_name)

            assert uuid == latest_bundle.uuid, "task has been re-executed unexpectedly!"
            return True

        def _serialize_bool(bool_value: bool) -> str:
            if isinstance(bool_value, str):
                return bool_value
            if not isinstance(bool_value, bool):
                raise TypeError('Value "{}" has type "{}" instead of bool.'.format(str(bool_value), str(type(bool_value))))
            return str(bool_value)

        import argparse
        _parser = argparse.ArgumentParser(prog='Validate container no execution', description='')
        _parser.add_argument("--bundle-name", dest="bundle_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--uuid", dest="uuid", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--context-name", dest="context_name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--s3-url", dest="s3_url", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=1)
        _parsed_args = vars(_parser.parse_args())
        _output_files = _parsed_args.pop("_output_paths", [])

        _outputs = validate_container_no_execution(**_parsed_args)

        _outputs = [_outputs]

        _output_serializers = [
            _serialize_bool,

        ]

        import os
        for idx, output_file in enumerate(_output_files):
            try:
                os.makedirs(os.path.dirname(output_file))
            except OSError:
                pass
            with open(output_file, 'w') as f:
                f.write(_output_serializers[idx](_outputs[idx]))
      image: docker.intuit.com/docker-rmt/python:3.8
    inputs:
      parameters:
      - {name: validate-container-execution-4-Output}
    outputs:
      artifacts:
      - {name: validate-container-no-execution-4-Output, path: /tmp/outputs/Output/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.6.5
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--bundle-name", {"inputValue": "bundle_name"}, "--uuid", {"inputValue":
          "uuid"}, "--context-name", {"inputValue": "context_name"}, "--s3-url", {"inputValue":
          "s3_url"}, "----output-paths", {"outputPath": "Output"}], "command": ["sh",
          "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
          ''disdat'' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
          --no-warn-script-location ''disdat'' --user) && \"$0\" \"$@\"", "sh", "-ec",
          "program_path=$(mktemp)\nprintf \"%s\" \"$0\" > \"$program_path\"\npython3
          -u \"$program_path\" \"$@\"\n", "def validate_container_no_execution(bundle_name,\n                                    uuid,\n                                    context_name,\n                                    s3_url):\n    from
          disdat import api\n    import os\n    import logging\n    func_name = validate_container_no_execution.__name__.upper()\n    os.system(''dsdt
          init'')\n    api.context(context_name)\n    api.remote(context_name, remote_context=context_name,
          remote_url=s3_url)\n    logging.basicConfig(level=logging.INFO)\n    logging.info(''{}
          - {}''.format(func_name, ''disdat initialized''))\n\n    api.pull(context_name,
          bundle_name=bundle_name, localize=False)\n    latest_bundle = api.get(context_name,
          bundle_name)\n\n    assert uuid == latest_bundle.uuid, \"task has been re-executed
          unexpectedly!\"\n    return True\n\ndef _serialize_bool(bool_value: bool)
          -> str:\n    if isinstance(bool_value, str):\n        return bool_value\n    if
          not isinstance(bool_value, bool):\n        raise TypeError(''Value \"{}\"
          has type \"{}\" instead of bool.''.format(str(bool_value), str(type(bool_value))))\n    return
          str(bool_value)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Validate
          container no execution'', description='''')\n_parser.add_argument(\"--bundle-name\",
          dest=\"bundle_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--uuid\",
          dest=\"uuid\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--context-name\",
          dest=\"context_name\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--s3-url\",
          dest=\"s3_url\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"----output-paths\",
          dest=\"_output_paths\", type=str, nargs=1)\n_parsed_args = vars(_parser.parse_args())\n_output_files
          = _parsed_args.pop(\"_output_paths\", [])\n\n_outputs = validate_container_no_execution(**_parsed_args)\n\n_outputs
          = [_outputs]\n\n_output_serializers = [\n    _serialize_bool,\n\n]\n\nimport
          os\nfor idx, output_file in enumerate(_output_files):\n    try:\n        os.makedirs(os.path.dirname(output_file))\n    except
          OSError:\n        pass\n    with open(output_file, ''w'') as f:\n        f.write(_output_serializers[idx](_outputs[idx]))\n"],
          "image": "docker.intuit.com/docker-rmt/python:3.8"}}, "inputs": [{"name":
          "bundle_name", "type": "String"}, {"name": "uuid", "type": "String"}, {"name":
          "context_name", "type": "String"}, {"name": "s3_url", "type": "String"}],
          "name": "Validate container no execution", "outputs": [{"name": "Output",
          "type": "Boolean"}]}', pipelines.kubeflow.org/component_ref: '{}', pipelines.kubeflow.org/arguments.parameters: '{"bundle_name":
          "divide_bundle_2", "context_name": "cascading_pipeline", "s3_url": "s3://mint-ai-disdatnoop-e2e-435945521637",
          "uuid": "{{inputs.parameters.validate-container-execution-4-Output}}"}'}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
